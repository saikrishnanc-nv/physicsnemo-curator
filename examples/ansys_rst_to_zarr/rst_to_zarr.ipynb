{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysicsNeMo-Curator Ansys Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section contains a tutorial for using PhysicsNeMo-Curator to prepare a training ready dataset from Ansys Solvers using the PyAnsys libraries.\n",
    "This tutorial will show how to use the PhysicsNeMo-Curator ETL pipeline to:\n",
    "\n",
    "1. Extract data from Ansys `.rst` files\n",
    "2. Transform the data into an optimized, AI model training ready format\n",
    "3. Write the transformed data to disk efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Installation of prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/physics_nemo/physicsnemo-curator-saikrishnanc-fork\n",
      "\u001b[33m  DEPRECATION: Setting PIP_CONSTRAINT will not affect build constraints in the future, pip 26.2 will enforce this behaviour change. A possible replacement is to specify build constraints using --build-constraint or PIP_BUILD_CONSTRAINT. To disable this warning without any build constraints set --use-feature=build-constraint or PIP_USE_FEATURE=\"build-constraint\".\u001b[0m\u001b[33m\n",
      "\u001b[0m  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ansys-dpf-core in /usr/local/lib/python3.12/dist-packages (0.14.2)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (1.26.4)\n",
      "Requirement already satisfied: pyvista>=0.44.2 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (0.46.4)\n",
      "Requirement already satisfied: vtk>=9.3.1 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (9.5.2)\n",
      "Requirement already satisfied: hydra-core>=1.3 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: zarr>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (3.1.5)\n",
      "Requirement already satisfied: numcodecs>=0.13.1 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (0.15.1)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (4.67.1)\n",
      "Requirement already satisfied: lasso-python==2.0.3 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (2.0.3)\n",
      "Requirement already satisfied: pre-commit in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (4.4.0)\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (25.9.0)\n",
      "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (0.14.5)\n",
      "Requirement already satisfied: interrogate in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (1.7.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (8.1.1)\n",
      "Requirement already satisfied: pytest-cov in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (7.0.0)\n",
      "Requirement already satisfied: pytest-mock in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (3.15.1)\n",
      "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (3.8.0)\n",
      "Requirement already satisfied: debugpy<1.9.0 in /usr/local/lib/python3.12/dist-packages (from physicsnemo_curator==0.0.1) (1.8.17)\n",
      "Requirement already satisfied: attrs<24.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (23.2.0)\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (3.15.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (2.3.3)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.10.0 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (5.24.1)\n",
      "Requirement already satisfied: psutil<6.0.0,>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (5.9.8)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (13.9.4)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (1.7.2)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from lasso-python==2.0.3->physicsnemo_curator==0.0.1) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly<6.0.0,>=5.10.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (9.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly<6.0.0,>=5.10.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (25.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.0.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.0.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (2.19.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.2.1->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.2.1->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (3.6.0)\n",
      "Requirement already satisfied: grpcio>=1.63.0 in /usr/local/lib/python3.12/dist-packages (from ansys-dpf-core) (1.75.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.0 in /usr/local/lib/python3.12/dist-packages (from ansys-dpf-core) (8.7.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from ansys-dpf-core) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ansys-dpf-core) (79.0.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.63.0->ansys-dpf-core) (4.15.0)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3->physicsnemo_curator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3->physicsnemo_curator==0.0.1) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3->physicsnemo_curator==0.0.1) (6.0.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=4.0->ansys-dpf-core) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (0.1.2)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from numcodecs>=0.13.1->physicsnemo_curator==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.0->lasso-python==2.0.3->physicsnemo_curator==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.10.7)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (11.3.0)\n",
      "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.8.2)\n",
      "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.2.5)\n",
      "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.12/dist-packages (from zarr>=3.1.2->physicsnemo_curator==0.0.1) (0.8.1.post1)\n",
      "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr>=3.1.2->physicsnemo_curator==0.0.1) (1.7.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->physicsnemo_curator==0.0.1) (8.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from black->physicsnemo_curator==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from black->physicsnemo_curator==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->physicsnemo_curator==0.0.1) (4.5.0)\n",
      "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.12/dist-packages (from black->physicsnemo_curator==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->numcodecs>=0.13.1->physicsnemo_curator==0.0.1) (1.17.3)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from interrogate->physicsnemo_curator==0.0.1) (0.4.6)\n",
      "Requirement already satisfied: py in /usr/local/lib/python3.12/dist-packages (from interrogate->physicsnemo_curator==0.0.1) (1.11.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from interrogate->physicsnemo_curator==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (2025.10.5)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit->physicsnemo_curator==0.0.1) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit->physicsnemo_curator==0.0.1) (2.6.15)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit->physicsnemo_curator==0.0.1) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit->physicsnemo_curator==0.0.1) (20.35.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit->physicsnemo_curator==0.0.1) (0.4.0)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit->physicsnemo_curator==0.0.1) (3.20.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.12/dist-packages (from pytest->physicsnemo_curator==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from pytest->physicsnemo_curator==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: coverage>=7.10.6 in /usr/local/lib/python3.12/dist-packages (from coverage[toml]>=7.10.6->pytest-cov->physicsnemo_curator==0.0.1) (7.12.0)\n",
      "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.12/dist-packages (from pytest-xdist->physicsnemo_curator==0.0.1) (2.1.1)\n",
      "Building wheels for collected packages: physicsnemo_curator\n",
      "  Building editable for physicsnemo_curator (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for physicsnemo_curator: filename=physicsnemo_curator-0.0.1-0.editable-py3-none-any.whl size=10025 sha256=153723a6c17117a0bd793619b397cc3a0df3ef930b837a7019fd20e3773cb970\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wxwlzu2r/wheels/08/d5/29/eff3d737047abd57a926b0b9dccfc9f8338d1480223f2b328a\n",
      "Successfully built physicsnemo_curator\n",
      "Installing collected packages: physicsnemo_curator\n",
      "  Attempting uninstall: physicsnemo_curator\n",
      "    Found existing installation: physicsnemo_curator 0.0.1\n",
      "    Uninstalling physicsnemo_curator-0.0.1:\n",
      "      Successfully uninstalled physicsnemo_curator-0.0.1\n",
      "Successfully installed physicsnemo_curator-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e \"../../[dev]\" ansys-dpf-core --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generate sample data\n",
    "\n",
    "We will generate mock .rst data, since using real data requires a license (instructions for adapting to real data are provided at the end of this tutorial).\n",
    "\n",
    "You can generate mock data by running the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generating Mock Thermal Simulation Data\n",
      "============================================================\n",
      "\n",
      "Generating: thermal_sim_001\n",
      "  Description: Central heat source with radial heat dissipation\n",
      "  Resolution: (12, 12, 12)\n",
      "Saved simulation to: mock_thermal_data/thermal_sim_001.rst\n",
      "  Nodes: 1728\n",
      "  Temperature range: 294.9 - 395.5 K\n",
      "\n",
      "Generating: thermal_sim_002\n",
      "  Description: Linear temperature gradient\n",
      "  Resolution: (15, 10, 10)\n",
      "Saved simulation to: mock_thermal_data/thermal_sim_002.rst\n",
      "  Nodes: 1500\n",
      "  Temperature range: 295.9 - 355.5 K\n",
      "\n",
      "Generating: thermal_sim_003\n",
      "  Description: Hot and cold boundary conditions\n",
      "  Resolution: (10, 10, 15)\n",
      "Saved simulation to: mock_thermal_data/thermal_sim_003.rst\n",
      "  Nodes: 1500\n",
      "  Temperature range: 244.5 - 354.6 K\n",
      "\n",
      "Generating: thermal_sim_004\n",
      "  Description: Multiple localized heat sources\n",
      "  Resolution: (14, 14, 8)\n",
      "Saved simulation to: mock_thermal_data/thermal_sim_004.rst\n",
      "  Nodes: 1568\n",
      "  Temperature range: 293.5 - 374.6 K\n",
      "\n",
      "Generating: thermal_sim_005\n",
      "  Description: Periodic heating pattern\n",
      "  Resolution: (16, 16, 10)\n",
      "Saved simulation to: mock_thermal_data/thermal_sim_005.rst\n",
      "  Nodes: 2560\n",
      "  Temperature range: 245.3 - 354.5 K\n",
      "\n",
      "============================================================\n",
      "Data generation complete!\n",
      "Generated 5 thermal simulation files (.rst) in: mock_thermal_data\n",
      "============================================================\n",
      "\n",
      "These mock .rst files can be processed by the ETL pipeline.\n",
      "To use real Ansys .rst files, you would need:\n",
      "  - Ansys installation (2021 R1+)\n",
      "  - Valid Ansys license\n",
      "  - PyDPF-Core configured with DPF Server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1242: RuntimeWarning: divide by zero encountered in divide\n",
      "  a = -(dx2)/(dx1 * (dx1 + dx2))\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1242: RuntimeWarning: invalid value encountered in divide\n",
      "  a = -(dx2)/(dx1 * (dx1 + dx2))\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1243: RuntimeWarning: divide by zero encountered in divide\n",
      "  b = (dx2 - dx1) / (dx1 * dx2)\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1243: RuntimeWarning: invalid value encountered in divide\n",
      "  b = (dx2 - dx1) / (dx1 * dx2)\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1244: RuntimeWarning: divide by zero encountered in divide\n",
      "  c = dx1 / (dx2 * (dx1 + dx2))\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1244: RuntimeWarning: invalid value encountered in divide\n",
      "  c = dx1 / (dx2 * (dx1 + dx2))\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1250: RuntimeWarning: invalid value encountered in add\n",
      "  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1259: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:1266: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n"
     ]
    }
   ],
   "source": [
    "import pickle  # noqa: F811\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_3d_mesh(nx: int, ny: int, nz: int, \n",
    "                     domain_size: tuple = (1.0, 1.0, 1.0)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a regular 3D mesh grid.\n",
    "    \n",
    "    Args:\n",
    "        nx, ny, nz: Number of nodes in x, y, z directions\n",
    "        domain_size: Physical size of the domain in meters (x, y, z)\n",
    "    \n",
    "    Returns:\n",
    "        coordinates: (N, 3) array of node coordinates\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, domain_size[0], nx)\n",
    "    y = np.linspace(0, domain_size[1], ny)\n",
    "    z = np.linspace(0, domain_size[2], nz)\n",
    "    \n",
    "    X, Y, Z = np.meshgrid(x, y, z, indexing='ij')\n",
    "    coordinates = np.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def generate_temperature_field(coordinates: np.ndarray, \n",
    "                                scenario: str = \"heat_source\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a temperature field based on different thermal scenarios.\n",
    "    \n",
    "    Args:\n",
    "        coordinates: (N, 3) array of node coordinates\n",
    "        scenario: Type of thermal scenario\n",
    "    \n",
    "    Returns:\n",
    "        temperature: (N,) array of temperature values in Kelvin\n",
    "    \"\"\"\n",
    "    x, y, z = coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]\n",
    "    \n",
    "    if scenario == \"heat_source\":\n",
    "        # Heat source at the center\n",
    "        center = np.array([0.5, 0.5, 0.5])\n",
    "        distance = np.sqrt((x - center[0])**2 + (y - center[1])**2 + (z - center[2])**2)\n",
    "        temperature = 300 + 100 * np.exp(-10 * distance**2)\n",
    "        \n",
    "    elif scenario == \"linear_gradient\":\n",
    "        # Linear temperature gradient in x-direction\n",
    "        temperature = 300 + 50 * x\n",
    "        \n",
    "    elif scenario == \"hot_cold_sides\":\n",
    "        # Hot on left, cold on right\n",
    "        temperature = 350 - 100 * x\n",
    "        \n",
    "    elif scenario == \"corner_heating\":\n",
    "        # Multiple heat sources at corners\n",
    "        corners = [\n",
    "            np.array([0.1, 0.1, 0.1]),\n",
    "            np.array([0.9, 0.9, 0.9])\n",
    "        ]\n",
    "        temperature = np.ones(len(coordinates)) * 300\n",
    "        for corner in corners:\n",
    "            distance = np.sqrt((x - corner[0])**2 + (y - corner[1])**2 + (z - corner[2])**2)\n",
    "            temperature += 80 * np.exp(-20 * distance**2)\n",
    "    \n",
    "    elif scenario == \"periodic_heating\":\n",
    "        # Periodic heating pattern\n",
    "        temperature = 300 + 50 * np.sin(4 * np.pi * x) * np.cos(4 * np.pi * y)\n",
    "    \n",
    "    else:\n",
    "        # Default uniform temperature\n",
    "        temperature = np.ones(len(coordinates)) * 300\n",
    "    \n",
    "    # Add some random noise to make it more realistic\n",
    "    temperature += np.random.normal(0, 2, len(coordinates))\n",
    "    \n",
    "    return temperature\n",
    "\n",
    "\n",
    "def generate_heat_flux(coordinates: np.ndarray, \n",
    "                       temperature: np.ndarray,\n",
    "                       thermal_conductivity: float = 50.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate heat flux vectors based on temperature gradients.\n",
    "    \n",
    "    Uses a simplified approach: flux ∝ -∇T (Fourier's law)\n",
    "    \n",
    "    Args:\n",
    "        coordinates: (N, 3) array of node coordinates\n",
    "        temperature: (N,) array of temperatures\n",
    "        thermal_conductivity: Material thermal conductivity (W/m·K)\n",
    "    \n",
    "    Returns:\n",
    "        heat_flux: (N, 3) array of heat flux vectors (W/m²)\n",
    "    \"\"\"\n",
    "    # Compute approximate temperature gradients using finite differences\n",
    "    # This is a simplified approach for mock data\n",
    "    \n",
    "    n_nodes = len(coordinates)\n",
    "    heat_flux = np.zeros((n_nodes, 3))\n",
    "    \n",
    "    # Simple gradient approximation based on position\n",
    "    x, y, z = coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]\n",
    "    \n",
    "    # Approximate gradients (this is mock data, so we use a simple approach)\n",
    "    # In reality, this would require proper finite element calculations\n",
    "    dT_dx = np.gradient(temperature.reshape(-1), x.reshape(-1))[0] if len(x) > 1 else 0\n",
    "    dT_dy = np.gradient(temperature.reshape(-1), y.reshape(-1))[0] if len(y) > 1 else 0\n",
    "    dT_dz = np.gradient(temperature.reshape(-1), z.reshape(-1))[0] if len(z) > 1 else 0\n",
    "    \n",
    "    # Fourier's law: q = -k * ∇T\n",
    "    heat_flux[:, 0] = -thermal_conductivity * dT_dx\n",
    "    heat_flux[:, 1] = -thermal_conductivity * dT_dy\n",
    "    heat_flux[:, 2] = -thermal_conductivity * dT_dz\n",
    "    \n",
    "    # Add some randomness to make it more realistic\n",
    "    heat_flux += np.random.normal(0, 50, heat_flux.shape)\n",
    "    \n",
    "    return heat_flux\n",
    "\n",
    "\n",
    "def generate_thermal_simulation(simulation_name: str,\n",
    "                                mesh_resolution: tuple = (10, 10, 10),\n",
    "                                scenario: str = \"heat_source\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a complete thermal simulation dataset.\n",
    "    \n",
    "    Args:\n",
    "        simulation_name: Name identifier for this simulation\n",
    "        mesh_resolution: Number of nodes in (x, y, z)\n",
    "        scenario: Thermal scenario type\n",
    "    \n",
    "    Returns:\n",
    "        data: Dictionary containing all simulation data\n",
    "    \"\"\"\n",
    "    # Generate mesh\n",
    "    coordinates = generate_3d_mesh(*mesh_resolution)\n",
    "    n_nodes = len(coordinates)\n",
    "    n_elements = (mesh_resolution[0] - 1) * (mesh_resolution[1] - 1) * (mesh_resolution[2] - 1)\n",
    "    \n",
    "    # Generate temperature field\n",
    "    temperature = generate_temperature_field(coordinates, scenario)\n",
    "    \n",
    "    # Generate heat flux\n",
    "    heat_flux = generate_heat_flux(coordinates, temperature)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"simulation_name\": simulation_name,\n",
    "        \"num_nodes\": n_nodes,\n",
    "        \"num_elements\": max(n_elements, 1),  # Ensure at least 1\n",
    "        \"mesh_resolution\": mesh_resolution,\n",
    "        \"scenario\": scenario,\n",
    "        \"units\": \"SI\",\n",
    "        \"temperature_units\": \"Kelvin\",\n",
    "        \"heat_flux_units\": \"W/m^2\",\n",
    "        \"coordinate_units\": \"meters\",\n",
    "        \"time_step\": 1,\n",
    "        \"analysis_type\": \"Steady-State Thermal\",\n",
    "        \"solver\": \"Mock Thermal Solver v1.0\",\n",
    "        \"temperature_min\": float(temperature.min()),\n",
    "        \"temperature_max\": float(temperature.max()),\n",
    "        \"temperature_mean\": float(temperature.mean()),\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"coordinates\": coordinates,\n",
    "        \"temperature\": temperature,\n",
    "        \"heat_flux\": heat_flux,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def save_simulation(data: Dict[str, Any], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save simulation data to disk.\n",
    "    \n",
    "    Args:\n",
    "        data: Simulation data dictionary\n",
    "        output_path: Path to save the data\n",
    "    \"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved simulation to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate 5 different thermal simulation datasets.\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"mock_thermal_data\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Generating Mock Thermal Simulation Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define 5 different simulation scenarios\n",
    "    simulations = [\n",
    "        {\n",
    "            \"name\": \"thermal_sim_001\",\n",
    "            \"resolution\": (12, 12, 12),\n",
    "            \"scenario\": \"heat_source\",\n",
    "            \"description\": \"Central heat source with radial heat dissipation\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thermal_sim_002\",\n",
    "            \"resolution\": (15, 10, 10),\n",
    "            \"scenario\": \"linear_gradient\",\n",
    "            \"description\": \"Linear temperature gradient\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thermal_sim_003\",\n",
    "            \"resolution\": (10, 10, 15),\n",
    "            \"scenario\": \"hot_cold_sides\",\n",
    "            \"description\": \"Hot and cold boundary conditions\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thermal_sim_004\",\n",
    "            \"resolution\": (14, 14, 8),\n",
    "            \"scenario\": \"corner_heating\",\n",
    "            \"description\": \"Multiple localized heat sources\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thermal_sim_005\",\n",
    "            \"resolution\": (16, 16, 10),\n",
    "            \"scenario\": \"periodic_heating\",\n",
    "            \"description\": \"Periodic heating pattern\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Generate each simulation\n",
    "    for sim_config in simulations:\n",
    "        print(f\"\\nGenerating: {sim_config['name']}\")\n",
    "        print(f\"  Description: {sim_config['description']}\")\n",
    "        print(f\"  Resolution: {sim_config['resolution']}\")\n",
    "        \n",
    "        data = generate_thermal_simulation(\n",
    "            simulation_name=sim_config['name'],\n",
    "            mesh_resolution=sim_config['resolution'],\n",
    "            scenario=sim_config['scenario']\n",
    "        )\n",
    "        \n",
    "        # Save to disk with .rst extension (mimicking real Ansys format)\n",
    "        output_path = output_dir / f\"{sim_config['name']}.rst\"\n",
    "        save_simulation(data, output_path)\n",
    "        \n",
    "        print(f\"  Nodes: {data['metadata']['num_nodes']}\")\n",
    "        print(f\"  Temperature range: {data['metadata']['temperature_min']:.1f} - \"\n",
    "              f\"{data['metadata']['temperature_max']:.1f} K\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data generation complete!\")\n",
    "    print(f\"Generated {len(simulations)} thermal simulation files (.rst) in: {output_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThese mock .rst files can be processed by the ETL pipeline.\")\n",
    "    print(\"To use real Ansys .rst files, you would need:\")\n",
    "    print(\"  - Ansys installation (2021 R1+)\")\n",
    "    print(\"  - Valid Ansys license\")\n",
    "    print(\"  - PyDPF-Core configured with DPF Server\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Implement a datasource\n",
    "\n",
    "We will implement a datasource to read the .rst files we just produced.\n",
    "Instructions are provided in the docstring to adapt this to real ANSYS .rst files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # noqa: F811\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class RstDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    DataSource for reading thermal simulation .rst files.\n",
    "    \n",
    "    This implementation reads mock .rst files (pickled Python data) for tutorial purposes.\n",
    "    \n",
    "    ============================================================================\n",
    "    USING REAL ANSYS .RST FILES:\n",
    "    ============================================================================\n",
    "    To adapt this for real Ansys .rst files, you need:\n",
    "    \n",
    "    1. Install Ansys (2021 R1 or later)\n",
    "    2. Have a valid Ansys license\n",
    "    3. Install PyDPF-Core: pip install ansys-dpf-core\n",
    "    4. Replace the read_file() method with PyDPF code:\n",
    "    \n",
    "        from ansys.dpf import core as dpf\n",
    "        \n",
    "        def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "            filepath = self.input_dir / f\"{filename}.rst\"\n",
    "            model = dpf.Model(str(filepath))\n",
    "            \n",
    "            # Extract mesh and temperature data\n",
    "            mesh = model.metadata.meshed_region\n",
    "            coords = np.array(mesh.nodes.coordinates_field.data)\n",
    "            \n",
    "            # Extract temperature (for thermal analysis)\n",
    "            temp_op = model.results.temperature()\n",
    "            temp_fields = temp_op.outputs.fields_container()\n",
    "            temperature = np.array(temp_fields[0].data)\n",
    "            \n",
    "            # Extract heat flux if available\n",
    "            flux_op = model.results.heat_flux()\n",
    "            flux_fields = flux_op.outputs.fields_container()\n",
    "            heat_flux = np.array(flux_fields[0].data)\n",
    "            \n",
    "            return {\n",
    "                \"coordinates\": coords,\n",
    "                \"temperature\": temperature,\n",
    "                \"heat_flux\": heat_flux,\n",
    "                \"metadata\": {...},\n",
    "                \"filename\": filename\n",
    "            }\n",
    "    \n",
    "    For more info: https://dpf.docs.pyansys.com/\n",
    "    ============================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, input_dir: str):\n",
    "        super().__init__(cfg)\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            raise FileNotFoundError(f\"Input directory {self.input_dir} does not exist\") \n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Find all .rst files in the input directory.\"\"\"\n",
    "        rst_files = list(self.input_dir.glob(\"*.rst\"))\n",
    "        filenames = [f.stem for f in rst_files]\n",
    "        self.logger.info(f\"Found {len(filenames)} .rst files to process\")\n",
    "        return sorted(filenames)\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Read a mock thermal .rst file.\n",
    "        \n",
    "        For this tutorial, .rst files are pickled Python dictionaries containing:\n",
    "        - coordinates: (N, 3) array of node positions\n",
    "        - temperature: (N,) array of temperature values in Kelvin\n",
    "        - heat_flux: (N, 3) array of heat flux vectors in W/m²\n",
    "        - metadata: dict with simulation information\n",
    "        \n",
    "        Returns data in a format ready for transformation.\n",
    "        \"\"\"\n",
    "        filepath = self.input_dir / f\"{filename}.rst\"\n",
    "        self.logger.info(f\"Reading thermal simulation from: {filepath}\")\n",
    "\n",
    "        # Load the pickled thermal data\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)  # noqa: S301\n",
    "        \n",
    "        # Add filename for tracking\n",
    "        data[\"filename\"] = filename\n",
    "        \n",
    "        # Log basic info\n",
    "        self.logger.info(f\"  Loaded {data['metadata']['num_nodes']} nodes\")\n",
    "        self.logger.info(f\"  Temperature range: {data['metadata']['temperature_min']:.1f} - \"\n",
    "                        f\"{data['metadata']['temperature_max']:.1f} K\")\n",
    "        self.logger.info(f\"  Analysis type: {data['metadata']['analysis_type']}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _get_output_path(self, filename: str) -> Path:\n",
    "        raise NotImplementedError(\"RstDataSource only supports reading\")\n",
    "\n",
    "    def _write_impl_temp_file(self, data: Dict[str, Any], output_path: Path) -> None:\n",
    "        raise NotImplementedError(\"RstDataSource only supports reading\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Implement the transformation\n",
    "\n",
    "To write to Zarr, we have to convert the in-memory data to a format that's ready to be written to be Zarr.\n",
    "\n",
    "You can do that by following the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "from physicsnemo_curator.etl.data_transformations import DataTransformation\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class RstToZarrTransformation(DataTransformation):\n",
    "    \"\"\"\n",
    "    Transform thermal simulation RST data into Zarr-optimized format.\n",
    "    \n",
    "    This transformation takes thermal analysis data (coordinates, temperature, heat_flux)\n",
    "    and prepares it for efficient storage in Zarr format with:\n",
    "    - Appropriate chunking for parallel access\n",
    "    - Compression optimized for floating-point scientific data\n",
    "    - Type conversion to float32 for storage efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, chunk_size: int = 1000, compression_level: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the transformation.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            chunk_size: Number of points per chunk (affects I/O performance)\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.compressor = zarr.codecs.BloscCodec(\n",
    "            cname=\"zstd\",\n",
    "            clevel=compression_level,\n",
    "            shuffle=zarr.codecs.BloscShuffle.shuffle,\n",
    "        )\n",
    "\n",
    "    def transform(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transform thermal RST data into Zarr-ready format.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary containing:\n",
    "                - coordinates: (N, 3) array of node positions\n",
    "                - temperature: (N,) array of temperature values\n",
    "                - heat_flux: (N, 3) array of heat flux vectors\n",
    "                - metadata: simulation metadata dict\n",
    "                - filename: source filename\n",
    "        \n",
    "        Returns:\n",
    "            zarr_data: Dictionary with arrays prepared for Zarr writing\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Transforming thermal data from {data['filename']}...\")\n",
    "\n",
    "        # Extract numpy arrays from the data source\n",
    "        coords = data['coordinates']\n",
    "        temperature = data['temperature']\n",
    "        heat_flux = data['heat_flux']\n",
    "        \n",
    "        num_points = len(coords)\n",
    "        \n",
    "        # Determine chunk size (do not exceed total points)\n",
    "        chunk_points = min(self.chunk_size, num_points)\n",
    "        \n",
    "        self.logger.info(f\"  Processing {num_points} nodes with chunk size {chunk_points}\")\n",
    "\n",
    "        # Prepare Zarr-ready data structure\n",
    "        zarr_data = {\n",
    "            'coordinates': {},\n",
    "            'temperature': {},\n",
    "            'heat_flux': {},\n",
    "            'metadata': data['metadata'].copy()  # Copy to avoid modifying original\n",
    "        }\n",
    "\n",
    "        # 1. Transform Coordinates (N, 3)\n",
    "        zarr_data['coordinates'] = {\n",
    "            'data': coords.astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressors': (self.compressor,),\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # 2. Transform Temperature (N,) - scalar field\n",
    "        zarr_data['temperature'] = {\n",
    "            'data': temperature.astype(np.float32),\n",
    "            'chunks': (chunk_points,),\n",
    "            'compressors': (self.compressor,),\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # 3. Transform Heat Flux (N, 3) - vector field\n",
    "        zarr_data['heat_flux'] = {\n",
    "            'data': heat_flux.astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressors': (self.compressor,),\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "        \n",
    "        # Add curation metadata\n",
    "        zarr_data['metadata']['curator_chunk_size'] = chunk_points\n",
    "        zarr_data['metadata']['compression_type'] = 'zstd'\n",
    "        zarr_data['metadata']['compression_level'] = self.compressor.clevel\n",
    "        \n",
    "        self.logger.info(f\"  Transformation complete for {data['filename']}\")\n",
    "        \n",
    "        return zarr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Implement the Zarr sink\n",
    "\n",
    "The Zarr sink takes in the transformed data, and writes it to disk in an efficient manner.\n",
    "This can be achieved by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import zarr  # noqa: F811\n",
    "from zarr.storage import LocalStore\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class ZarrDataSource(DataSource):\n",
    "    \"\"\"DataSource for writing to Zarr stores.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, output_dir: str):\n",
    "        \"\"\"Initialize the Zarr data source.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            output_dir: Directory to write Zarr stores\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        # Create output directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def _get_output_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get the output path for a given filename.\n",
    "\n",
    "        Args:\n",
    "            filename: Name of the file to process\n",
    "\n",
    "        Returns:\n",
    "            Path object representing the output location.\n",
    "        \"\"\"\n",
    "        return self.output_dir / f\"{filename}.zarr\"\n",
    "\n",
    "    def _write_impl_temp_file(self, data: Dict[str, Any], output_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Implement actual data writing logic to a temporary Zarr store.\n",
    "\n",
    "        This method is called by the write() method to write the data to a temporary Zarr store.\n",
    "        The data is written to a temporary Zarr store and then renamed to the final output path.\n",
    "        This is to improve the robustness of the write operation.\n",
    "\n",
    "        Args:\n",
    "            data: Transformed data from RstToZarrTransformation\n",
    "            output_path: Path where data should be written (may be temporary)\n",
    "        \"\"\"\n",
    "        # Create Zarr store\n",
    "        self.logger.info(f\"Creating Zarr store: {output_path}\")\n",
    "        store = LocalStore(output_path)\n",
    "        root = zarr.open_group(store=store, mode=\"w\")\n",
    "\n",
    "        # Store metadata as root attributes\n",
    "        if \"metadata\" in data:\n",
    "            for key, value in data[\"metadata\"].items():\n",
    "                # Convert numpy types to Python types for JSON serialization\n",
    "                if hasattr(value, \"item\"):  # numpy scalar\n",
    "                    value = value.item()\n",
    "                root.attrs[key] = value\n",
    "            data.pop(\"metadata\")\n",
    "\n",
    "        # Write all arrays from the transformation\n",
    "        for array_name, array_info in data.items():\n",
    "            root.create_array(\n",
    "                name=array_name,\n",
    "                data=array_info[\"data\"],\n",
    "                chunks=array_info[\"chunks\"],\n",
    "                compressors=array_info[\"compressors\"],  # Already a tuple from transformation\n",
    "            )\n",
    "\n",
    "        # Add some store-level metadata\n",
    "        root.attrs[\"zarr_format\"] = 3\n",
    "        root.attrs[\"created_by\"] = \"physicsnemo-curator-tutorial\"\n",
    "\n",
    "        self.logger.info(\"Successfully created Zarr store\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        \"\"\"Check if we should skip writing this store.\n",
    "\n",
    "        Args:\n",
    "            filename: Base filename to check\n",
    "\n",
    "        Returns:\n",
    "            True if store should be skipped (already exists)\n",
    "        \"\"\"\n",
    "        store_path = self.output_dir / f\"{filename}.zarr\"\n",
    "        exists = store_path.exists()\n",
    "\n",
    "        if exists:\n",
    "            self.logger.info(f\"Skipping {filename} - Zarr store already exists\")\n",
    "            return True\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Create a script to run the pipeline\n",
    "PhysicsNeMo-Curator has a central orchestrator, to help you orchestrate your ETL pipeline. However, we need to create a script to instantiate the various components defined above, pass it to the orchestrator (along with multiprocessing context) and run the pipeline. The following code snippet can also be found in ./run_etl.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "# PhysicsNeMo Curator Imports\n",
    "from physicsnemo_curator.etl.etl_orchestrator import ETLOrchestrator\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "from physicsnemo_curator.utils import utils as curator_utils\n",
    "\n",
    "\n",
    "@hydra.main(version_base=\"1.3\", config_path=\"./config\", config_name=\"st_pydpf_config\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Main ETL pipeline execution.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Hydra configuration object loaded from YAML\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Ansys Thermal RST to Zarr ETL Pipeline\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Setup Multiprocessing\n",
    "    curator_utils.setup_multiprocessing()\n",
    "    \n",
    "    # 2. Parse Processing Config\n",
    "    processing_config = ProcessingConfig(**cfg.etl.processing)\n",
    "    print(f\"Processing with {processing_config.num_processes} processes\")\n",
    "\n",
    "    # 3. Instantiate ETL Components\n",
    "    print(\"\\nInitializing ETL components...\")\n",
    "    \n",
    "    # Source: Reads thermal RST files\n",
    "    source = instantiate(cfg.etl.source, processing_config)\n",
    "    print(f\"  Source: {cfg.etl.source._target_}\")\n",
    "    print(f\"  Input directory: {cfg.etl.source.input_dir}\")\n",
    "    \n",
    "    # Sink: Writes Zarr stores\n",
    "    sink = instantiate(cfg.etl.sink, processing_config)\n",
    "    print(f\"  Sink: {cfg.etl.sink._target_}\")\n",
    "    print(f\"  Output directory: {cfg.etl.sink.output_dir}\")\n",
    "\n",
    "    # Transformations: RST to Zarr conversion\n",
    "    cfgs = {k: {\"_args_\": [processing_config]} for k in cfg.etl.transformations.keys()}\n",
    "    transformations = instantiate(cfg.etl.transformations, **cfgs)\n",
    "    print(f\"  Transformations: {list(cfg.etl.transformations.keys())}\")\n",
    "\n",
    "    # 4. Run ETL Orchestrator\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Starting ETL Pipeline...\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    orchestrator = ETLOrchestrator(\n",
    "        source=source,\n",
    "        sink=sink,\n",
    "        transformations=transformations,\n",
    "        processing_config=processing_config,\n",
    "    )\n",
    "    orchestrator.run()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ETL Pipeline Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Create a config\n",
    "\n",
    "Now we'll tie everything together with a configuration file and run the complete pipeline. This config is present in [./config/rst_to_zarr.yaml](./config/rst_to_zarr.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "etl:\n",
    "  processing:\n",
    "    num_processes: 4  # Adjust based on available CPU cores\n",
    "    args: {}\n",
    "\n",
    "  # Source: Read thermal simulation .rst files\n",
    "  source:\n",
    "    _target_: rst_data_source.RstDataSource\n",
    "    _convert_: all\n",
    "    input_dir: ???  # Specify via CLI: etl.source.input_dir=path/to/rst/files\n",
    "\n",
    "  # Transformations: Convert RST thermal data to Zarr-ready format\n",
    "  transformations:\n",
    "    rst_to_zarr:\n",
    "      _target_: rst_to_zarr_transformation.RstToZarrTransformation\n",
    "      _convert_: all\n",
    "      chunk_size: 1000          # Points per chunk (tune for your data size)\n",
    "      compression_level: 5      # Zstd level (1-9, higher = more compression)\n",
    "\n",
    "  # Sink: Write Zarr stores to disk\n",
    "  sink:\n",
    "    _target_: zarr_data_source.ZarrDataSource\n",
    "    _convert_: all\n",
    "    output_dir: ???  # Specify via CLI: etl.sink.output_dir=path/to/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Run the ETL Pipeline\n",
    "\n",
    "Now you can run the complete pipeline using the run_etl.py file we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Ansys Thermal RST to Zarr ETL Pipeline\n",
      "======================================================================\n",
      "Processing with 4 processes\n",
      "\n",
      "Initializing ETL components...\n",
      "  Source: rst_data_source.RstDataSource\n",
      "  Input directory: mock_thermal_data\n",
      "  Sink: zarr_data_source.ZarrDataSource\n",
      "  Output directory: output_zarr\n",
      "  Transformations: ['rst_to_zarr']\n",
      "\n",
      "======================================================================\n",
      "Starting ETL Pipeline...\n",
      "======================================================================\n",
      "\n",
      "[2025-12-11 21:43:08,360][physicsnemo_curator.utils.utils][INFO] - Starting ETL pipeline\n",
      "[2025-12-11 21:43:08,364][RstDataSource][INFO] - Found 5 .rst files to process\n",
      "Processing files: 100%|█████████████████████████| 5/5 [00:00<00:00,  5.50file/s]\n",
      "[2025-12-11 21:43:09,274][physicsnemo_curator.utils.utils][INFO] - \n",
      "Processing Summary:\n",
      "[2025-12-11 21:43:09,274][physicsnemo_curator.utils.utils][INFO] - Number of processes: 4\n",
      "[2025-12-11 21:43:09,274][physicsnemo_curator.utils.utils][INFO] - Total wall clock time: 0.91 seconds\n",
      "\n",
      "======================================================================\n",
      "ETL Pipeline Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python run_etl.py --config-dir ./ \\\n",
    "  --config-name rst_to_zarr \\\n",
    "  etl.source.input_dir=mock_thermal_data \\\n",
    "  etl.sink.output_dir=output_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Execution:**\n",
    "\n",
    "1. **Processing Phase**: For each file:\n",
    "   - RstDataSource reads the rst file\n",
    "   - RstToZarrTransformation converts it to Zarr-optimized format\n",
    "   - ZarrDataSource writes the result to a `.zarr` store\n",
    "\n",
    "3. **Parallel Execution**: Uses 4 processes to handle multiple files simultaneously\n",
    "\n",
    "4. **Output**: Creates individual Zarr stores for each input file\n",
    "\n",
    "**Expected Output Structure:**\n",
    "\n",
    "After running, you'll have:\n",
    "\n",
    "```bash\n",
    "output_zarr/\n",
    "├── run_001.zarr/\n",
    "│   ├── coordinates/\n",
    "│   ├── temperature/\n",
    "│   ├── velocity/\n",
    "│   ├── velocity_magnitude/\n",
    "│   └── .zattrs (metadata)\n",
    "├── run_002.zarr/\n",
    "├── run_003.zarr/\n",
    "├── run_004.zarr/\n",
    "└── run_005.zarr/\n",
    "```\n",
    "\n",
    "**Verify the Results:**\n",
    "\n",
    "You can inspect the output using this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Zarr Store Contents: thermal_sim_001.zarr\n",
      "============================================================\n",
      "\n",
      "📊 Arrays in store:\n",
      "  - temperature\n",
      "  - heat_flux\n",
      "  - coordinates\n",
      "\n",
      "📏 Array Details:\n",
      "  heat_flux:\n",
      "    Shape: (1728, 3)\n",
      "    Dtype: float32\n",
      "    Chunks: (1000, 3)\n",
      "  temperature:\n",
      "    Shape: (1728,)\n",
      "    Dtype: float32\n",
      "    Chunks: (1000,)\n",
      "  coordinates:\n",
      "    Shape: (1728, 3)\n",
      "    Dtype: float32\n",
      "    Chunks: (1000, 3)\n",
      "\n",
      "📋 Metadata:\n",
      "  simulation_name: thermal_sim_001\n",
      "  num_nodes: 1728\n",
      "  num_elements: 1331\n",
      "  mesh_resolution: [12, 12, 12]\n",
      "  scenario: heat_source\n",
      "  units: SI\n",
      "  temperature_units: Kelvin\n",
      "  heat_flux_units: W/m^2\n",
      "  coordinate_units: meters\n",
      "  time_step: 1\n",
      "  analysis_type: Steady-State Thermal\n",
      "  solver: Mock Thermal Solver v1.0\n",
      "  temperature_min: 294.86\n",
      "  temperature_max: 395.47\n",
      "  temperature_mean: 312.98\n",
      "  curator_chunk_size: 1000\n",
      "  compression_type: zstd\n",
      "  compression_level: 5\n",
      "  zarr_format: 3\n",
      "  created_by: physicsnemo-curator-tutorial\n",
      "\n",
      "🌡️  Temperature Sample (first 5 values):\n",
      "  [301.25833 302.9406  303.90695 300.95227 302.8256 ]\n",
      "\n",
      "💨 Heat Flux Sample (first 3 vectors):\n",
      "  [[       -inf        -inf -1071.0347 ]\n",
      " [       -inf        -inf  -894.73193]\n",
      " [       -inf        -inf  -952.71234]]\n"
     ]
    }
   ],
   "source": [
    "import zarr  # noqa: F811\n",
    "\n",
    "# Open one of the Zarr stores\n",
    "store = zarr.open(\"./output_zarr/thermal_sim_001.zarr\", mode=\"r\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Zarr Store Contents: thermal_sim_001.zarr\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all arrays\n",
    "print(\"\\n📊 Arrays in store:\")\n",
    "for key in store.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Show array shapes and dtypes\n",
    "print(\"\\n📏 Array Details:\")\n",
    "for key in store.keys():\n",
    "    array = store[key]\n",
    "    print(f\"  {key}:\")\n",
    "    print(f\"    Shape: {array.shape}\")\n",
    "    print(f\"    Dtype: {array.dtype}\")\n",
    "    print(f\"    Chunks: {array.chunks}\")\n",
    "\n",
    "# Show metadata\n",
    "print(\"\\n📋 Metadata:\")\n",
    "for key, value in store.attrs.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Show sample data from temperature\n",
    "print(\"\\n🌡️  Temperature Sample (first 5 values):\")\n",
    "print(f\"  {store['temperature'][:5]}\")\n",
    "\n",
    "print(\"\\n💨 Heat Flux Sample (first 3 vectors):\")\n",
    "print(f\"  {store['heat_flux'][:3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
