{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysicsNeMo-Curator Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section contains a tutorial for using PhysicsNeMo-Curator to create a dataset.\n",
    "This tutorial will show how to use the PhysicsNeMo-Curator ETL pipeline to:\n",
    "\n",
    "1. Extract physics simulation data from a dataset\n",
    "2. Transform the data into an optimized, AI model training ready format\n",
    "3. Write the transformed data to disk efficiently\n",
    "\n",
    "## Create a dataset\n",
    "\n",
    "PhysicsNeMo-Curator works only with well-defined formats and schemas.\n",
    "As such, defining that is a necessary first step.\n",
    "Next, we'll create a custom dataset, in a custom schema, format and storage system.\n",
    "\n",
    "### Step 1: Define the schema, format, storage system\n",
    "\n",
    "For this tutorial, we'll create a simple simulation dataset using:\n",
    "\n",
    "**Format**: HDF5\n",
    "**Storage**: Local filesystem\n",
    "**Schema**: This is the structure for each simulation run\n",
    "(xyz indicates the run number):\n",
    "\n",
    "```bash\n",
    "run_xyz.h5\n",
    "├── /fields/\n",
    "│   ├── temperature          # Dataset: (N,) float32 - scalar temperature field\n",
    "│   └── velocity             # Dataset: (N, 3) float32 - 3D velocity vectors\n",
    "├── /geometry/\n",
    "│   └── coordinates          # Dataset: (N, 3) float32 - spatial coordinates (x,y,z)\n",
    "└── /metadata/\n",
    "    ├── timestamp            # Attribute: string - when simulation was run\n",
    "    ├── num_points           # Attribute: int - number of data points\n",
    "    ├── temperature_units    # Attribute: string - \"Kelvin\"\n",
    "    ├── velocity_units       # Attribute: string - \"m/s\"\n",
    "    └── simulation_params/   # Group containing simulation parameters\n",
    "        └── total_time      # Attribute: float - total simulation time\n",
    "```\n",
    "\n",
    "**Data Description**:\n",
    "\n",
    "- Each simulation run is one HDF5 file\n",
    "- `N` represents the number of spatial points in the simulation (varies per case)\n",
    "- Temperature is a scalar field representing thermal distribution\n",
    "- Velocity is a 3D vector field representing fluid flow\n",
    "- Coordinates define the spatial location of each data point\n",
    "- Metadata includes simulation parameters and units for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate random data\n",
    "\n",
    "We'll create a small script to generate 5 simulation runs with random data.\n",
    "Each file will contain about 1000 data points to keep it lightweight.\n",
    "\n",
    "First, we need to setup the environment.\n",
    "Let's install the dependencies necessary for this tutorial. This includes:\n",
    "\n",
    "1. Installing the `PhysicsNeMo-Curator` package itself\n",
    "2. h5py, numpy and zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e \"../../../[dev]\" h5py numpy zarr --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate some random data, that adheres to the schema we've developed above.\n",
    "The below code snippet can also be found as a script in [./generate_sample_data.py](./generate_sample_data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample physics simulation dataset...\n",
      "Created tutorial_data/run_001.h5 with 1 data points\n",
      "Created tutorial_data/run_002.h5 with 2 data points\n",
      "Created tutorial_data/run_003.h5 with 3 data points\n",
      "Created tutorial_data/run_004.h5 with 4 data points\n",
      "Created tutorial_data/run_005.h5 with 5 data points\n",
      "\n",
      "Dataset generation complete!\n",
      "Created 5 HDF5 files in the 'tutorial_data/' directory\n",
      "Each file contains ~1000 data points with temperature and velocity fields\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n",
    "import numpy as np  # noqa: F811\n",
    "\n",
    "\n",
    "def generate_simulation_data(num_points=1000):\n",
    "    \"\"\"Generate random simulation data for one run.\"\"\"\n",
    "\n",
    "    # Generate random 3D coordinates in a unit cube\n",
    "    coordinates = np.random.uniform(-1.0, 1.0, size=(num_points, 3)).astype(np.float32)\n",
    "\n",
    "    # Generate temperature field (scalar, range 250-350 K)\n",
    "    temperature = np.random.uniform(250.0, 350.0, size=num_points).astype(np.float32)\n",
    "\n",
    "    # Generate velocity field (3D vectors, range -5 to 5 m/s)\n",
    "    velocity = np.random.uniform(-5.0, 5.0, size=(num_points, 3)).astype(np.float32)\n",
    "\n",
    "    return coordinates, temperature, velocity\n",
    "\n",
    "def create_hdf5_file(run_number, output_dir=\"tutorial_data\"):\n",
    "    \"\"\"Create one HDF5 file for a simulation run.\"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate data\n",
    "    coordinates, temperature, velocity = generate_simulation_data(run_number)\n",
    "    num_points = len(coordinates)\n",
    "\n",
    "    # Create HDF5 file\n",
    "    filename = f\"run_{run_number:03d}.h5\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        # Create groups\n",
    "        fields_group = f.create_group('fields')\n",
    "        geometry_group = f.create_group('geometry')\n",
    "        metadata_group = f.create_group('metadata')\n",
    "        sim_params_group = metadata_group.create_group('simulation_params')\n",
    "\n",
    "        # Store field data\n",
    "        fields_group.create_dataset('temperature', data=temperature)\n",
    "        fields_group.create_dataset('velocity', data=velocity)\n",
    "\n",
    "        # Store geometry data\n",
    "        geometry_group.create_dataset('coordinates', data=coordinates)\n",
    "\n",
    "        # Store metadata attributes\n",
    "        metadata_group.attrs['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        metadata_group.attrs['num_points'] = num_points\n",
    "        metadata_group.attrs['temperature_units'] = 'Kelvin'\n",
    "        metadata_group.attrs['velocity_units'] = 'm/s'\n",
    "\n",
    "        # Store simulation parameters\n",
    "        sim_params_group.attrs['total_time'] = np.random.uniform(1.0, 10.0)  # Random simulation time\n",
    "\n",
    "    print(f\"Created {filepath} with {num_points} data points\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate sample dataset with 5 simulation runs.\"\"\"\n",
    "    print(\"Generating sample physics simulation dataset...\")\n",
    "\n",
    "    # Generate 5 runs\n",
    "    for run_num in range(1, 6):\n",
    "        create_hdf5_file(run_num)\n",
    "\n",
    "    print(\"\\nDataset generation complete!\")\n",
    "    print(\"Created 5 HDF5 files in the 'tutorial_data/' directory\")\n",
    "    print(\"Each file contains ~1000 data points with temperature and velocity fields\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `tutorial_data/` directory with 5 files.\n",
    "Now we're ready to implement the ETL pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the ETL pipeline\n",
    "\n",
    "The PhysicsNeMo-Curator ETL pipeline consists of four main components:\n",
    "\n",
    "1. **DataSource** - Handles both reading input data AND writing output data\n",
    "(serves as both source and sink)\n",
    "2. **DataTransformation** - Transforms data from one format to another\n",
    "3. **DatasetValidator** - Validates input data structure and content (optional)\n",
    "4. **ParallelProcessor** - Orchestrates the processing files in parallel\n",
    "\n",
    "**For this tutorial, our specific pipeline will be:**\n",
    "\n",
    "1. **H5DataSource** (source) - Reads HDF5 files and extracts raw data\n",
    "2. **H5ToZarrTransformation** - Converts it to a Zarr-compatible format\n",
    "3. **ZarrDataSource** (sink) - Writes the transformed data to Zarr stores\n",
    "4. **TutorialValidator** - Validates our HDF5 input files\n",
    "\n",
    "The data flow:\n",
    "`HDF5 files → H5DataSource → H5ToZarrTransformation → ZarrDataSource → Zarr stores`\n",
    "\n",
    "**Important**:\n",
    "Notice that we use different DataSource classes for reading and writing -\n",
    "one specialized for HDF5 input, another for Zarr output.\n",
    "This shows how you can mix and match different data sources in the same pipeline.\n",
    "However, notice also that `DataSource` serves dual purposes -\n",
    "one instance of a class can read your input data, while another\n",
    "instance of the same class can write your output data.\n",
    "This design allows the same class to handle both ends of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Implement dataset validation\n",
    "\n",
    "First, we'll implement validation to ensure our input HDF5 files\n",
    "meet the required schema and format.\n",
    "This runs at the beginning of the pipeline to catch issues early.\n",
    "The below code snippet can also be found as a script in [./tutorial_validator.py](./tutorial_validator.py).\n",
    "\n",
    "NOTE: The intention is to run this as part of the pipeline, and not as a standalone script.\n",
    "If you'd like to run it as a separate script, please modify the execution accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import h5py  # noqa: F811\n",
    "\n",
    "from physicsnemo_curator.etl.dataset_validators import (\n",
    "    DatasetValidator,\n",
    "    ValidationError,\n",
    "    ValidationLevel,\n",
    ")\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class TutorialValidator(DatasetValidator):\n",
    "    \"\"\"Validator for HDF5 physics simulation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, input_dir: str, validation_level: str = \"fields\"):\n",
    "        \"\"\"Initialize the validator.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            input_dir: Directory containing HDF5 files to validate\n",
    "            validation_level: \"structure\" or \"fields\"\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.validation_level = ValidationLevel(validation_level)\n",
    "\n",
    "        # Define our expected schema\n",
    "        self.required_groups = ['/fields', '/geometry', '/metadata', '/metadata/simulation_params']\n",
    "        self.required_datasets = {\n",
    "            '/fields/temperature': {'shape_dims': 1, 'dtype': 'float'},\n",
    "            '/fields/velocity': {'shape_dims': 2, 'expected_cols': 3, 'dtype': 'float'},\n",
    "            '/geometry/coordinates': {'shape_dims': 2, 'expected_cols': 3, 'dtype': 'float'}\n",
    "        }\n",
    "        self.required_attributes = {\n",
    "            '/metadata': ['timestamp', 'num_points', 'temperature_units', 'velocity_units'],\n",
    "            '/metadata/simulation_params': ['total_time']\n",
    "        }\n",
    "\n",
    "    def validate(self) -> List[ValidationError]:\n",
    "        \"\"\"Validate the entire dataset.\n",
    "\n",
    "        Returns:\n",
    "            List of validation errors (empty if validation passes)\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check if input directory exists\n",
    "        if not self.input_dir.exists():\n",
    "            errors.append(ValidationError(\n",
    "                path=self.input_dir,\n",
    "                message=f\"Input directory does not exist: {self.input_dir}\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "            return errors\n",
    "\n",
    "        # Find all HDF5 files\n",
    "        h5_files = list(self.input_dir.glob(\"*.h5\"))\n",
    "\n",
    "        if not h5_files:\n",
    "            errors.append(ValidationError(\n",
    "                path=self.input_dir,\n",
    "                message=\"No HDF5 files found in input directory\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "            return errors\n",
    "\n",
    "        # Validate each file\n",
    "        for h5_file in h5_files:\n",
    "            file_errors = self.validate_single_item(h5_file)\n",
    "            errors.extend(file_errors)\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def validate_single_item(self, item: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate a single HDF5 file.\n",
    "\n",
    "        Args:\n",
    "            item: Path to HDF5 file to validate\n",
    "\n",
    "        Returns:\n",
    "            List of validation errors for this file\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "\n",
    "        try:\n",
    "            with h5py.File(item, 'r') as f:\n",
    "                # Structure validation\n",
    "                errors.extend(self._validate_structure(f, item))\n",
    "\n",
    "                # Field validation (if requested and structure is valid)\n",
    "                if self.validation_level == ValidationLevel.FIELDS and not errors:\n",
    "                    errors.extend(self._validate_fields(f, item))\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append(ValidationError(\n",
    "                path=item,\n",
    "                message=f\"Failed to open HDF5 file: {str(e)}\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_structure(self, f: h5py.File, file_path: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate HDF5 file structure.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check required groups exist\n",
    "        errors.extend([\n",
    "            ValidationError(\n",
    "                path=file_path,\n",
    "                message=f\"Missing required group: {group_path}\",\n",
    "                level=self.validation_level,\n",
    "            )\n",
    "            for group_path in self.required_groups\n",
    "            if group_path not in f\n",
    "        ])\n",
    "\n",
    "        # Check required datasets exist and have correct structure\n",
    "        for dataset_path, requirements in self.required_datasets.items():\n",
    "            if dataset_path not in f:\n",
    "                errors.append(\n",
    "                    ValidationError(\n",
    "                        path=file_path,\n",
    "                        message=f\"Missing required dataset: {dataset_path}\",\n",
    "                        level=self.validation_level,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            dataset = f[dataset_path]\n",
    "\n",
    "            # Check dimensions\n",
    "            if len(dataset.shape) != requirements[\"shape_dims\"]:\n",
    "                errors.append(\n",
    "                    ValidationError(\n",
    "                        path=file_path,\n",
    "                        message=f\"Dataset {dataset_path} has wrong dimensions: expected {requirements['shape_dims']}D, got {len(dataset.shape)}D\",\n",
    "                        level=self.validation_level,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Check column count for 2D arrays\n",
    "            if \"expected_cols\" in requirements and len(dataset.shape) >= 2:\n",
    "                if dataset.shape[1] != requirements[\"expected_cols\"]:\n",
    "                    errors.append(\n",
    "                        ValidationError(\n",
    "                            path=file_path,\n",
    "                            message=f\"Dataset {dataset_path} has wrong number of columns: expected {requirements['expected_cols']}, got {dataset.shape[1]}\",\n",
    "                            level=self.validation_level,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Check required attributes exist\n",
    "        errors.extend([\n",
    "            ValidationError(\n",
    "                path=file_path,\n",
    "                message=f\"Missing required attribute: {group_path}@{attr_name}\",\n",
    "                level=self.validation_level,\n",
    "            )\n",
    "            for group_path, attr_list in self.required_attributes.items()\n",
    "            if group_path in f\n",
    "            for attr_name in attr_list\n",
    "            if attr_name not in f[group_path].attrs\n",
    "        ])\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_fields(self, f: h5py.File, file_path: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate field data content.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check that datasets have consistent sizes\n",
    "        if '/fields/temperature' in f and '/geometry/coordinates' in f:\n",
    "            temp_size = f['/fields/temperature'].shape[0]\n",
    "            coord_size = f['/geometry/coordinates'].shape[0]\n",
    "\n",
    "            if temp_size != coord_size:\n",
    "                errors.append(ValidationError(\n",
    "                    path=file_path,\n",
    "                    message=f\"Inconsistent data sizes: temperature has {temp_size} points, coordinates has {coord_size} points\",\n",
    "                    level=self.validation_level\n",
    "                ))\n",
    "\n",
    "        # Check for reasonable data ranges\n",
    "        if '/fields/temperature' in f:\n",
    "            temp_data = f['/fields/temperature'][:]\n",
    "            if temp_data.min() < 0 or temp_data.max() > 10000:  # Kelvin range check\n",
    "                errors.append(ValidationError(\n",
    "                    path=file_path,\n",
    "                    message=f\"Temperature data out of reasonable range: [{temp_data.min():.1f}, {temp_data.max():.1f}] K\",\n",
    "                    level=self.validation_level\n",
    "                ))\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement data source\n",
    "\n",
    "We'll create a simple DataSource that reads our HDF5 files. \n",
    "The below code snippet can also be found as a script in [./h5_data_source.py](./h5_data_source.py).\n",
    "\n",
    "**Note**: This DataSource only implements reading from HDF5 files, since the idea here is for the source to be read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import h5py\n",
    "import numpy as np  # noqa: F811\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class H5DataSource(DataSource):\n",
    "    \"\"\"DataSource for reading HDF5 physics simulation files.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, input_dir: str):\n",
    "        \"\"\"Initialize the H5 data source.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            input_dir: Directory containing input HDF5 files\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            raise FileNotFoundError(f\"Input directory {self.input_dir} does not exist\")\n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Get list of HDF5 files to process.\n",
    "\n",
    "        Returns:\n",
    "            List of filenames (without extension) to process\n",
    "        \"\"\"\n",
    "        # Find all .h5 files and return their base names\n",
    "        h5_files = list(self.input_dir.glob(\"*.h5\"))\n",
    "        filenames = [f.stem for f in h5_files]  # Remove .h5 extension\n",
    "\n",
    "        self.logger.info(f\"Found {len(filenames)} HDF5 files to process\")\n",
    "        return sorted(filenames)\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Read one HDF5 file and extract all data.\n",
    "\n",
    "        Args:\n",
    "            filename: Base filename (without extension)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing extracted data and metadata\n",
    "        \"\"\"\n",
    "        filepath = self.input_dir / f\"{filename}.h5\"\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "        self.logger.warning(f\"Reading {filepath}\")\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        with h5py.File(filepath, \"r\") as f:\n",
    "            # Read field data\n",
    "            data[\"temperature\"] = np.array(f[\"fields/temperature\"])\n",
    "            data[\"velocity\"] = np.array(f[\"fields/velocity\"])\n",
    "\n",
    "            # Read geometry data\n",
    "            data[\"coordinates\"] = np.array(f[\"geometry/coordinates\"])\n",
    "\n",
    "            # Read metadata\n",
    "            metadata = dict(f[\"metadata\"].attrs.items())\n",
    "\n",
    "            data[\"metadata\"] = metadata\n",
    "            data[\"filename\"] = filename\n",
    "\n",
    "        self.logger.warning(f\"Loaded data with {len(data['temperature'])} points\")\n",
    "        return data\n",
    "\n",
    "    def _get_output_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get the final output path for a given filename.\n",
    "\n",
    "        Args:\n",
    "            filename: Name of the file to process\n",
    "\n",
    "        Returns:\n",
    "            Path object representing the final output location\n",
    "        \"\"\"\n",
    "        return NotImplementedError(\"H5DataSource only supports reading\")\n",
    "\n",
    "    def _write_impl_temp_file(\n",
    "        self,\n",
    "        data: Dict[str, Any],\n",
    "        output_path: Path,\n",
    "    ) -> None:\n",
    "        \"\"\"Not implemented - this DataSource only reads.\"\"\"\n",
    "        raise NotImplementedError(\"H5DataSource only supports reading\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        \"\"\"Never skip files for reading.\"\"\"\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement transformations\n",
    "\n",
    "Now we'll create a transformation that converts our HDF5 data into a format optimized for Zarr storage. \n",
    "The below code snippet can also be found as a script in [./h5_to_zarr_transformation.py](./h5_to_zarr_transformation.py).\n",
    "\n",
    "**Key Points About This Transformation:**\n",
    "\n",
    "1. **Zarr Optimization**: Prepares data with chunks, compression, and proper dtypes for efficient Zarr storage\n",
    "\n",
    "2. **Chunking Strategy**: Uses configurable chunk sizes optimized for the data size\n",
    "\n",
    "3. **Compression**: Uses zstd compression with Blosc for good performance/size balance\n",
    "\n",
    "4. **Derived data**: Adds derived fields like velocity magnitude and temperature statistics\n",
    "\n",
    "5. **Metadata**: Adds technical metadata about chunking and compression settings\n",
    "\n",
    "The output format is specifically designed to be consumed by a `ZarrDataSource` that will create the actual Zarr store structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np  # noqa: F811\n",
    "from numcodecs import Blosc\n",
    "\n",
    "from physicsnemo_curator.etl.data_transformations import DataTransformation\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class H5ToZarrTransformation(DataTransformation):\n",
    "    \"\"\"Transform HDF5 data into Zarr-optimized format.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, chunk_size: int = 500, compression_level: int = 3):\n",
    "        \"\"\"Initialize the transformation.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            chunk_size: Chunk size for Zarr arrays (number of points per chunk)\n",
    "            compression_level: Compression level (1-9, higher = more compression)\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.compression_level = compression_level\n",
    "\n",
    "        # Set up compression\n",
    "        self.compressor = Blosc(\n",
    "            cname='zstd',  # zstd compression algorithm\n",
    "            clevel=compression_level,\n",
    "            shuffle=Blosc.SHUFFLE\n",
    "        )\n",
    "\n",
    "    def transform(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Transform HDF5 data to Zarr-optimized format.\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary from H5DataSource.read_file()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with Zarr-optimized arrays and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Transforming {data['filename']} for Zarr storage\")\n",
    "\n",
    "        # Get the number of points to determine chunking\n",
    "        num_points = len(data['temperature'])\n",
    "\n",
    "        # Calculate optimal chunks (don't exceed chunk_size)\n",
    "        chunk_points = min(self.chunk_size, num_points)\n",
    "\n",
    "        # Prepare arrays that will be written to Zarr stores\n",
    "        zarr_data = {\n",
    "            'temperature': {},\n",
    "            'velocity': {},\n",
    "            'coordinates': {},\n",
    "            'velocity_magnitude': {},\n",
    "        }\n",
    "\n",
    "        # Temperature field (1D array)\n",
    "        zarr_data['temperature'] = {\n",
    "            'data': data['temperature'].astype(np.float32),\n",
    "            'chunks': (chunk_points,),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Velocity field (2D array: points x 3 components)\n",
    "        zarr_data['velocity'] = {\n",
    "            'data': data['velocity'].astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Coordinates (2D array: points x 3 dimensions)\n",
    "        zarr_data['coordinates'] = {\n",
    "            'data': data['coordinates'].astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Add some computed metadata useful for Zarr to existing metadata\n",
    "        metadata = data['metadata']\n",
    "        metadata['num_points'] = num_points\n",
    "        metadata['chunk_size'] = chunk_points\n",
    "        metadata['compression'] = 'zstd'\n",
    "        metadata['compression_level'] = self.compression_level\n",
    "\n",
    "        # Also add some simple derived fields\n",
    "        # Temperature statistics\n",
    "        metadata['temperature_min'] = float(np.min(data['temperature']))\n",
    "        metadata['temperature_max'] = float(np.max(data['temperature']))\n",
    "        metadata['temperature_mean'] = float(np.mean(data['temperature']))\n",
    "\n",
    "        # Velocity magnitude\n",
    "        velocity_magnitude = np.linalg.norm(data['velocity'], axis=1)\n",
    "        zarr_data['velocity_magnitude'] = {\n",
    "            'data': velocity_magnitude.astype(np.float32),\n",
    "            'chunks': (chunk_points,),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "        metadata['velocity_max'] = float(np.max(velocity_magnitude))\n",
    "        zarr_data['metadata'] = metadata\n",
    "\n",
    "        return zarr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement sink\n",
    "\n",
    "Now we'll create a DataSource that writes to Zarr stores.\n",
    "Each simulation run will be stored as a separate Zarr store for efficient individual access.\n",
    "The following code snippet can also be found in [./zarr_data_source.py](./zarr_data_source.py).\n",
    "\n",
    "**Key Points About This Sink:**\n",
    "\n",
    "1. **Individual Stores**: Each simulation run gets its own `.zarr` directory for efficient access\n",
    "\n",
    "2. **Optimized Storage**: Uses the chunking and compression settings from the transformation\n",
    "\n",
    "3. **Complete Metadata**: Stores all metadata as Zarr attributes for easy access\n",
    "\n",
    "4. **Overwrite Control**: Configurable overwrite behavior for reprocessing workflows\n",
    "\n",
    "5. **Write-Only**: This sink only writes data\n",
    "\n",
    "6. **Temp-then-rename**: All sinks in PhysicsNeMo-Curator are designed to write to a temp file first, which then gets renamed to the actual filename. This is to increase robustness in case of job interruptions. Writing to a file might take a while, but renames are atomic in most filesystems. Therefore, the overwrite control mentioned above will be much more robust with this design pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import zarr\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class ZarrDataSource(DataSource):\n",
    "    \"\"\"DataSource for writing to Zarr stores.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, output_dir: str):\n",
    "        \"\"\"Initialize the Zarr data source.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            output_dir: Directory to write Zarr stores\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.output_dir = Path(output_dir)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def _get_output_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get the output path for a given filename.\n",
    "\n",
    "        Args:\n",
    "            filename: Name of the file to process\n",
    "\n",
    "        Returns:\n",
    "            Path object representing the output location.\n",
    "        \"\"\"\n",
    "        return self.output_dir / f\"{filename}.zarr\"\n",
    "\n",
    "    def _write_impl_temp_file(self, data: Dict[str, Any], output_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Implement actual data writing logic to a temporary Zarr store.\n",
    "\n",
    "        This method is called by the write() method to write the data to a temporary Zarr store.\n",
    "        The data is written to a temporary Zarr store and then renamed to the final output path.\n",
    "        This is to improve the robustness of the write operation.\n",
    "\n",
    "        Args:\n",
    "            data: Transformed data from H5ToZarrTransformation\n",
    "            output_path: Path where data should be written (may be temporary)\n",
    "        \"\"\"\n",
    "        # Create Zarr store\n",
    "        self.logger.info(f\"Creating Zarr store: {output_path}\")\n",
    "        store = zarr.DirectoryStore(output_path)\n",
    "        root = zarr.group(store=store)\n",
    "\n",
    "        # Store metadata as root attributes\n",
    "        if \"metadata\" in data:\n",
    "            for key, value in data[\"metadata\"].items():\n",
    "                # Convert numpy types to Python types for JSON serialization\n",
    "                if hasattr(value, \"item\"):  # numpy scalar\n",
    "                    value = value.item()\n",
    "                root.attrs[key] = value\n",
    "            data.pop(\"metadata\")\n",
    "\n",
    "        # Write all arrays from the transformation\n",
    "        for array_name, array_info in data.items():\n",
    "            root.create_dataset(\n",
    "                array_name,\n",
    "                data=array_info[\"data\"],\n",
    "                chunks=array_info[\"chunks\"],\n",
    "                compressor=array_info[\"compressor\"],\n",
    "                dtype=array_info[\"dtype\"],\n",
    "            )\n",
    "\n",
    "        # Add some store-level metadata\n",
    "        root.attrs[\"zarr_format\"] = 2\n",
    "        root.attrs[\"created_by\"] = \"physicsnemo-curator-tutorial\"\n",
    "\n",
    "        # Something weird is happening here.\n",
    "        # If this error occurs, the stores are created and we move to the next one.\n",
    "        # If this error does NOT occur, we seem to skip all the remaining files.\n",
    "        # Debug with Alexey.\n",
    "        self.logger.info(\"Successfully created Zarr store\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        \"\"\"Check if we should skip writing this store.\n",
    "\n",
    "        Args:\n",
    "            filename: Base filename to check\n",
    "\n",
    "        Returns:\n",
    "            True if store should be skipped (already exists)\n",
    "        \"\"\"\n",
    "        store_path = self.output_dir / f\"{filename}.zarr\"\n",
    "        exists = store_path.exists()\n",
    "\n",
    "        if exists:\n",
    "            self.logger.info(f\"Skipping {filename} - Zarr store already exists\")\n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a script to run the pipeline\n",
    "\n",
    "PhysicsNeMo-Curator has a central orchestrator, to help you orchestrate your ETL pipeline. However, we need to create a script to instantiate the various components defined above, pass it to the orchestrator (along with multiprocessing context) and run the pipeline.\n",
    "The following code snippet can also be found in [./run_etl.py](./run_etl.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa: F811\n",
    "import sys  # noqa: F811\n",
    "\n",
    "# Add current directory to sys.path so Hydra can import tutorial modules\n",
    "sys.path.insert(0, os.path.dirname(__file__))\n",
    "\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from physicsnemo_curator.etl.etl_orchestrator import ETLOrchestrator\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "from physicsnemo_curator.utils import utils as curator_utils\n",
    "\n",
    "\n",
    "@hydra.main(version_base=\"1.3\", config_path=\".\", config_name=\"tutorial_config\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "    \"\"\"Run the HDF5 to Zarr tutorial ETL pipeline.\n",
    "\n",
    "    This function:\n",
    "    1. Sets up multiprocessing context\n",
    "    2. Creates the processing config\n",
    "    3. Instantiates all components (source, sink, transformations, validator)\n",
    "    4. Passes them to the orchestrator\n",
    "    5. Runs the pipeline\n",
    "    \"\"\"\n",
    "    # Set multiprocessing start method\n",
    "    curator_utils.setup_multiprocessing()\n",
    "\n",
    "    # Create processing config with common settings\n",
    "    processing_config = ProcessingConfig(**cfg.etl.processing)\n",
    "\n",
    "    # Create and run validator (if configured)\n",
    "    validator = None\n",
    "    if \"validator\" in cfg.etl:\n",
    "        validator = instantiate(\n",
    "            cfg.etl.validator,\n",
    "            processing_config,\n",
    "            **{k: v for k, v in cfg.etl.source.items() if not k.startswith(\"_\")},\n",
    "        )\n",
    "\n",
    "    # Instantiate source\n",
    "    source = instantiate(cfg.etl.source, processing_config)\n",
    "\n",
    "    # Instantiate sink\n",
    "    sink = instantiate(cfg.etl.sink, processing_config)\n",
    "\n",
    "    # Instantiate transformations\n",
    "    # Need to pass processing_config to each transformation, see:\n",
    "    # https://hydra.cc/docs/advanced/instantiate_objects/overview/#recursive-instantiation\n",
    "    cfgs = {k: {\"_args_\": [processing_config]} for k in cfg.etl.transformations.keys()}\n",
    "    transformations = instantiate(cfg.etl.transformations, **cfgs)\n",
    "\n",
    "    # Create and run orchestrator with instantiated components\n",
    "    orchestrator = ETLOrchestrator(\n",
    "        source=source,\n",
    "        sink=sink,\n",
    "        transformations=transformations,\n",
    "        processing_config=processing_config,\n",
    "        validator=validator,\n",
    "    )\n",
    "    orchestrator.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a config\n",
    "\n",
    "Now we'll tie everything together with a configuration file to run the complete pipeline. This config is present in [./tutorial_config.yaml](./tutorial_config.yaml).\n",
    "\n",
    "```yaml\n",
    "# Tutorial ETL Pipeline Configuration\n",
    "# This demonstrates the complete H5 -> Zarr processing pipeline\n",
    "\n",
    "etl:\n",
    "  # Processing settings\n",
    "  processing:\n",
    "    num_processes: 2  # Use 2 processes for this small tutorial dataset\n",
    "    args: {}\n",
    "\n",
    "  # Validation (runs first)\n",
    "  validator:\n",
    "    _target_: tutorial_validator.TutorialValidator\n",
    "    _convert_: all\n",
    "    input_dir: ???  # Will be provided via command line\n",
    "    validation_level: \"fields\"  # Full validation including data content\n",
    "\n",
    "  # Source (reads HDF5 files)\n",
    "  source:\n",
    "    _target_: h5_data_source.H5DataSource\n",
    "    _convert_: all\n",
    "    input_dir: ???  # Will be provided via command line\n",
    "\n",
    "  # Transformations (convert to Zarr format)\n",
    "  transformations:\n",
    "    h5_to_zarr:\n",
    "      _target_: h5_to_zarr_transformation.H5ToZarrTransformation\n",
    "      _convert_: all\n",
    "      chunk_size: 500\n",
    "      compression_level: 3\n",
    "\n",
    "  # Sink (writes Zarr stores)\n",
    "  sink:\n",
    "    _target_: zarr_data_source.ZarrDataSource\n",
    "    _convert_: all\n",
    "    output_dir: ???  # Will be provided via command line\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the ETL Pipeline:**\n",
    "\n",
    "Now you can run the complete pipeline using the `run_etl.py` file we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-21 19:50:10,904][physicsnemo_curator.utils.utils][INFO] - Starting ETL pipeline\n",
      "[2025-11-21 19:50:10,918][H5DataSource][INFO] - Found 5 HDF5 files to process\n",
      "Processing files:   0%|                                 | 0/5 [00:00<?, ?file/s]Reading tutorial_data/run_004.h5\n",
      "Loaded data with 4 points\n",
      "Processing files:   0%|                                 | 0/5 [00:00<?, ?file/s]Reading tutorial_data/run_001.h5\n",
      "Loaded data with 1 points\n",
      "Reading tutorial_data/run_005.h5\n",
      "Loaded data with 5 points\n",
      "Processing files:  20%|█████                    | 1/5 [00:00<00:02,  1.98file/s]Reading tutorial_data/run_002.h5\n",
      "Loaded data with 2 points\n",
      "Reading tutorial_data/run_003.h5\n",
      "Loaded data with 3 points\n",
      "Processing files: 100%|█████████████████████████| 5/5 [00:00<00:00,  7.09file/s]\n",
      "[2025-11-21 19:50:11,625][physicsnemo_curator.utils.utils][INFO] - \n",
      "Processing Summary:\n",
      "[2025-11-21 19:50:11,625][physicsnemo_curator.utils.utils][INFO] - Number of processes: 2\n",
      "[2025-11-21 19:50:11,625][physicsnemo_curator.utils.utils][INFO] - Total wall clock time: 0.71 seconds\n"
     ]
    }
   ],
   "source": [
    "!python run_etl.py --config-dir ./ \\\n",
    "  --config-name tutorial_config \\\n",
    "  etl.source.input_dir=tutorial_data \\\n",
    "  etl.sink.output_dir=output_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Execution:**\n",
    "\n",
    "1. **Validation Phase**: The pipeline first validates all HDF5 files in `tutorial_data/`\n",
    "   - Checks file structure and schema compliance\n",
    "   - Validates data ranges and consistency\n",
    "   - Stops execution if any validation errors are found\n",
    "\n",
    "2. **Processing Phase**: For each validated file:\n",
    "   - H5DataSource reads the HDF5 file\n",
    "   - H5ToZarrTransformation converts it to Zarr-optimized format\n",
    "   - ZarrDataSource writes the result to a `.zarr` store\n",
    "\n",
    "3. **Parallel Execution**: Uses 2 processes to handle multiple files simultaneously\n",
    "\n",
    "4. **Output**: Creates individual Zarr stores for each input file\n",
    "\n",
    "**Expected Output Structure:**\n",
    "\n",
    "After running, you'll have:\n",
    "\n",
    "```bash\n",
    "output_zarr/\n",
    "├── run_001.zarr/\n",
    "│   ├── temperature/\n",
    "│   ├── velocity/\n",
    "│   ├── coordinates/\n",
    "│   ├── velocity_magnitude/\n",
    "│   └── .zattrs (metadata)\n",
    "├── run_002.zarr/\n",
    "├── run_003.zarr/\n",
    "├── run_004.zarr/\n",
    "└── run_005.zarr/\n",
    "```\n",
    "\n",
    "**Verify the Results:**\n",
    "\n",
    "You can inspect the output using this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays in store: ['coordinates', 'temperature', 'velocity', 'velocity_magnitude']\n",
      "Temperature data shape: (1,)\n",
      "Velocity data shape: (1, 3)\n",
      "Metadata: {'chunk_size': 1, 'compression': 'zstd', 'compression_level': 3, 'created_by': 'physicsnemo-curator-tutorial', 'num_points': 1, 'temperature_max': 255.14730834960938, 'temperature_mean': 255.14730834960938, 'temperature_min': 255.14730834960938, 'temperature_units': 'Kelvin', 'timestamp': '2025-11-21 19:47:47', 'velocity_max': 6.3494696617126465, 'velocity_units': 'm/s', 'zarr_format': 2}\n",
      "Temperature range: 255.1 - 255.1 K\n"
     ]
    }
   ],
   "source": [
    "import zarr  # noqa: F811\n",
    "\n",
    "# Open a Zarr store\n",
    "store = zarr.open(\"output_zarr/run_001.zarr\", mode=\"r\")\n",
    "\n",
    "print(\"Arrays in store:\", list(store.keys()))\n",
    "print(\"Temperature data shape:\", store[\"temperature\"].shape)\n",
    "print(\"Velocity data shape:\", store[\"velocity\"].shape)\n",
    "print(\"Metadata:\", dict(store.attrs))\n",
    "\n",
    "# Access the data\n",
    "temperature = store[\"temperature\"][:]\n",
    "print(f\"Temperature range: {temperature.min():.1f} - {temperature.max():.1f} K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
